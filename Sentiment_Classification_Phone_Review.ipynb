{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing python utilities for task execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:30:15.903500Z",
     "start_time": "2019-04-28T05:30:13.471213Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:32:38.274688Z",
     "start_time": "2019-04-28T05:32:38.203097Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('K8 Reviews v0.2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:32:43.298623Z",
     "start_time": "2019-04-28T05:32:43.283456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good but need updates and improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Worst mobile i have bought ever, Battery is dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when I will get my 10% cash back.... its alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The worst phone everThey have changed the last...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1             Good but need updates and improvements\n",
       "1          0  Worst mobile i have bought ever, Battery is dr...\n",
       "2          1  when I will get my 10% cash back.... its alrea...\n",
       "3          1                                               Good\n",
       "4          0  The worst phone everThey have changed the last..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check top 5 rows\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:32:51.815684Z",
     "start_time": "2019-04-28T05:32:51.811362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14675, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Size of the dataset - it has 14675 reviews and 2 fields as seen above\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7712\n",
       "1    6963\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:36:04.683408Z",
     "start_time": "2019-04-28T05:36:04.680131Z"
    },
    "collapsed": true
   },
   "source": [
    "## Using the following NLTK modules for text preprocessing\n",
    " - NLTK's Porter Stemmer for word stemming and WordNet Lemmatizer for lemmatization\n",
    " - NLTK's word tokenizer for splitting sentences into words\n",
    " - NLTK's stop word module for removing unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/howto/stem.html\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "# https://pythonspot.com/nltk-stop-words/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spell import correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'superb'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"superbbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:36:25.398321Z",
     "start_time": "2019-04-28T05:36:25.392269Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        corrected = correction(item)\n",
    "        stemmed.append(stemmer.stem(corrected))\n",
    "    return stemmed\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens, lemm):\n",
    "    lemmatized = []\n",
    "    for item in tokens:\n",
    "        corrected = correction(item)\n",
    "        lemmatized.append(lemm.lemmatize(corrected))\n",
    "    return lemmatized\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # remove non letters\n",
    "#     text = re.sub(r'\\b\\w{1,3}\\b', '', text) # remove letters with less than 3 chars\n",
    "    # tokenize (split into words)\n",
    "    tokens = nltk.word_tokenize(text)    \n",
    "    # stem\n",
    "    stems = lemmatize_tokens(tokens, lemm)\n",
    "#     stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Scikit Learn for vectorizing the text into numbers as Machine Learning algorithms only understands numbers\n",
    "- We use Count Vectorizer/Tf-Idf Vectorizer for this task. It takes various parameters in one go for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True, #We want only lower cased words during vectorization\n",
    "    stop_words = 'english', # Remove unwanted words from the sentences. Words like - is,the,if,it,etc are not required\n",
    "    max_features = 100 # we are only considering first 100 features for sentiment classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True, #We want only lower cased words during vectorization\n",
    "#     stop_words = 'english', # Remove unwanted words from the sentences. Words like - is,the,if,it,etc are not required\n",
    "    max_features = 200 # we are only considering first 100 features for sentiment classification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Applying the vectorizer to the list of all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:38:05.146101Z",
     "start_time": "2019-04-28T05:37:58.109312Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(reviews.review.tolist())\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we have transformed the text into numbers and extracted only top N features given by parameter 'max features' in vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T06:08:05.492972Z",
     "start_time": "2019-04-28T06:08:05.446713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.10707407,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:38:47.067667Z",
     "start_time": "2019-04-28T05:38:47.063593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14675, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features_nd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see some of the words considered in sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T06:09:25.620353Z",
     "start_time": "2019-04-28T06:09:25.577611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'adroit',\n",
       " 'after',\n",
       " 'all',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'an']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting the reviews into train and test sets\n",
    "- train set will be used for training the model to understand the data\n",
    "- test set will be used to check the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T06:19:28.130533Z",
     "start_time": "2019-04-28T06:19:28.083522Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(corpus_data_features_nd[0:len(reviews)]\n",
    "                                                 ,reviews.sentiment\n",
    "                                                 ,random_state = 42\n",
    "                                                 ,train_size = 0.75 #75:25 ratio\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm - Logistic Regression\n",
    " - Training the model with different regularization parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8367402562005997\n",
      "Accuracy for C=0.05: 0.8555464704279095\n",
      "Accuracy for C=0.25: 0.8629054238212047\n",
      "Accuracy for C=0.5: 0.8661760697737804\n",
      "Accuracy for C=1: 0.8661760697737804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c,class_weight='balanced')\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      1921\n",
      "           1       0.85      0.87      0.86      1748\n",
      "\n",
      "    accuracy                           0.87      3669\n",
      "   macro avg       0.87      0.87      0.87      3669\n",
      "weighted avg       0.87      0.87      0.87      3669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at 5 most discriminating words each for Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('awesome', 5.097339923909264)\n",
      "('best', 4.50820283429397)\n",
      "('excellent', 4.406626472360816)\n",
      "('great', 3.8414357212126307)\n",
      "('superb', 3.373699344815222)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        vectorizer.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(best_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('worst', -6.446337865274848)\n",
      "('not', -5.7554926264707555)\n",
      "('poor', -4.693562310440202)\n",
      "('waste', -3.4916882775863507)\n",
      "('bad', -3.3918666462054032)\n"
     ]
    }
   ],
   "source": [
    "for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking prediction on random reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>1</td>\n",
       "      <td>Battery problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>0</td>\n",
       "      <td>phone is good but it,s pries is to high 10000 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4608</th>\n",
       "      <td>0</td>\n",
       "      <td>Stock android not user friendly like earlier v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12730</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't buy this....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>0</td>\n",
       "      <td>yupprocessing is good....speed is good ...came...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                             review  Predicted\n",
       "2093           1                                    Battery problem          0\n",
       "6106           0  phone is good but it,s pries is to high 10000 ...          0\n",
       "4608           0  Stock android not user friendly like earlier v...          0\n",
       "12730          0                                 Don't buy this....          0\n",
       "3117           0  yupprocessing is good....speed is good ...came...          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_reviews = reviews.sample(50) # choosing random 15 reviews\n",
    "\n",
    "## Applying same transformation\n",
    "features = vectorizer.fit_transform(random_reviews.review.tolist())\n",
    "features = features.toarray()\n",
    "\n",
    "# Prediction for random reviews\n",
    "y_pred_random = lr.predict(features)\n",
    "\n",
    "random_reviews['Predicted'] = y_pred_random\n",
    "\n",
    "random_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training with different classifier algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    GaussianNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Nearest Neighbors: 0.7748705369310439\n",
      "Accuracy for Linear SVM: 0.8416462251294631\n",
      "Accuracy for RBF SVM: 0.8642681929681112\n",
      "Accuracy for Decision Tree: 0.7675115835377487\n",
      "Accuracy for Random Forest: 0.7274461706186972\n",
      "Accuracy for Neural Net: 0.8626328699918234\n",
      "Accuracy for Naive Bayes: 0.8048514581629872\n"
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Accuracy for %s: %s\" \n",
    "           % (name, clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_object = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "tokenizer_object.fit_on_texts(reviews.review.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722\n"
     ]
    }
   ],
   "source": [
    "#checking max length of review which is required during pad sequences\n",
    "\n",
    "max_length = max([len(s.split()) for s in reviews.review.tolist()])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13007\n"
     ]
    }
   ],
   "source": [
    "#vocabulary size\n",
    "\n",
    "vocab_size = len(tokenizer_object.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(reviews.review\n",
    "                                                 ,reviews.sentiment\n",
    "                                                 ,random_state = 42\n",
    "                                                 ,train_size = 0.70\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text to list of words/tokens\n",
    "X_train_tokens = tokenizer_object.texts_to_sequences(x_train)\n",
    "X_test_tokens = tokenizer_object.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds zero padding to make each sentence of same length (here max_length)\n",
    "X_train_pad = pad_sequences(X_train_tokens,maxlen = max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen = max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing keras utilities\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing LSTM model\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,EMBEDDING_DIM, input_length = max_length)) #Embedding layer\n",
    "model.add(LSTM(units=256, dropout = 0.3,recurrent_dropout=0.3)) #LSTM layer with dropout\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid')) #Fully connected output layer\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 722, 200)          2601400   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,069,625\n",
      "Trainable params: 3,069,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10272 samples, validate on 4403 samples\n",
      "Epoch 1/5\n",
      " - 235s - loss: 0.6930 - acc: 0.5227 - val_loss: 0.6919 - val_acc: 0.5249\n",
      "Epoch 2/5\n",
      " - 233s - loss: 0.6924 - acc: 0.5183 - val_loss: 0.6920 - val_acc: 0.5249\n",
      "Epoch 3/5\n",
      " - 232s - loss: 0.6924 - acc: 0.5258 - val_loss: 0.6922 - val_acc: 0.5249\n",
      "Epoch 4/5\n",
      " - 233s - loss: 0.6922 - acc: 0.5251 - val_loss: 0.6919 - val_acc: 0.5249\n",
      "Epoch 5/5\n",
      " - 232s - loss: 0.6921 - acc: 0.5258 - val_loss: 0.6951 - val_acc: 0.5249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5299707b8>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "model.fit(X_train_pad,y_train,batch_size=32,epochs=5,validation_data=(X_test_pad,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.69\n",
      "acc: 0.52\n"
     ]
    }
   ],
   "source": [
    "#Evaluating on test set\n",
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the word2vec model on the reviews\n",
    "\n",
    "wv_model = models.Word2Vec([s.split() for s in reviews.review.to_list()]\n",
    "                           ,min_count = 3 # Ignores all words with total frequency lower than this\n",
    "                           ,size = 200 #Dimensionality of the word vectors.\n",
    "                           ,window = 3 #Maximum distance between the current and predicted word within a sentence.\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of all the word : vector pairs\n",
    "embeddings_index = {}\n",
    "for w in wv_model.wv.vocab.keys():\n",
    "    embeddings_index[w] = wv_model.wv[w]\n",
    "\n",
    "#embeddings_index['Good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_object.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a embedding matrix which is required as weights in the embedding layer on LSTM model\n",
    "num_words = vocab_size\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "for word, i in tokenizer_object.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing LSTM model\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "        Embedding(vocab_size\n",
    "                ,EMBEDDING_DIM\n",
    "                ,weights = [embedding_matrix] #Supplied embedding matrix created from word vectors\n",
    "                ,input_length = max_length\n",
    "                ,trainable=False)\n",
    "         )\n",
    "\n",
    "model.add(LSTM(units=256, dropout = 0.3,recurrent_dropout=0.3))\n",
    "\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=32,epochs=5,validation_data=(X_test_pad,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways of improving the accuracy\n",
    " - Try different tokenizer (eg. spacy tokenizer)\n",
    " - use a different spell correction method\n",
    " - use word segmentation technique for separating the erroneous joined words\n",
    " - convert emojis into texts\n",
    " - remove contractions like \"wasn't\" to \"was not\", \"isn't\" to \"is not\",etc\n",
    " - convert smileys into text\n",
    " - use Fackbook AI's fasttext based word vectors representation as it operates on character instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
